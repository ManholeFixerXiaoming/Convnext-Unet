import sys

import convnext22 as convnext
import numpy
import torch
from torch import nn
from torchsummary import summary
from torchviz import make_dot
import torch.nn.functional as F
from timm.models.layers import trunc_normal_, DropPath
from timm.models.registry import register_model

# c = convnext.Block(1, drop_path=0., layer_scale_init_value=1e-6)


class cqm_unetLayer(nn.Module):  #：B,C,H,W
    def __init__(self, input_channel, drop_path1=0,padd=1, padd2=0):
        super(cqm_unetLayer, self).__init__()

        self.bn00 = nn.BatchNorm2d(10)
        self.bn0 = nn.BatchNorm2d(64)
        self.bn1 = nn.BatchNorm2d(128)
        self.bn3 = nn.BatchNorm2d(256)
        self.bn5 = nn.BatchNorm2d(512)
        self.bn7 = nn.BatchNorm2d(1024)
        self.bn9 = nn.BatchNorm2d(2048)
        self.conv1 = nn.Conv2d(input_channel,128,kernel_size=(3, 3), padding=padd, stride=1)
        # self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), padding=padd, stride=1)
        self.conv2 = convnext.Block(128, drop_path=drop_path1)
        self.down1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=padd2)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=padd, stride=1)
        # self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=padd, stride=1)
        self.conv4 = convnext.Block(256, drop_path=drop_path1)
        self.down2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=padd2)
        self.conv5 = nn.Conv2d(256, 512, kernel_size=(3, 3), padding=padd, stride=1)
        self.conv6 = convnext.Block(512, drop_path=drop_path1)
        self.down3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=padd2)
        self.conv7 = nn.Conv2d(512, 1024, kernel_size=(3, 3), padding=padd, stride=1)
        # self.conv8 = nn.Conv2d(512, 512, kernel_size=(3, 3), padding=padd, stride=1)
        self.conv8 = convnext.Block(1024, drop_path=drop_path1)
        self.down4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=padd2)
        self.conv9 = nn.Conv2d(1024, 2048, kernel_size=(3, 3), padding=padd, stride=1)
        # self.conv10 = nn.Conv2d(1024, 1024, kernel_size=(3, 3), padding=padd, stride=1)
        self.conv10 = convnext.Block(2048, drop_path=drop_path1)
        self.up1 = nn.ConvTranspose2d(2048, 1024, kernel_size=2, stride=2, padding=padd2)  # 这里注意，up后缩小一倍
        self.up_conv1 = nn.Conv2d(2048, 1024, kernel_size=(3, 3), padding=padd, stride=1)
        # self.up_conv2 = nn.Conv2d(512, 512, kernel_size=(3, 3), padding=padd, stride=1)
        self.up_conv2 = convnext.Block(1024, drop_path=drop_path1)
        self.up2 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2, padding=padd2)
        self.up_conv3 = nn.Conv2d(1024, 512, kernel_size=(3, 3), padding=padd, stride=1)
        # self.up_conv4 = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=padd, stride=1)
        self.up_conv4 = convnext.Block(512, drop_path=drop_path1)
        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=padd2)
        self.up_conv5 = nn.Conv2d(512, 256, kernel_size=(3, 3), padding=padd, stride=1)
        # self.up_conv6 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=padd, stride=1)
        self.up_conv6 = convnext.Block(256, drop_path=drop_path1)
        self.up4 = nn.ConvTranspose2d(256,128, kernel_size=2, stride=2, padding=padd2)
        self.up_conv7 = nn.Conv2d(256, 128, kernel_size=(3, 3), padding=padd, stride=1)
        # self.up_conv8 = nn.Conv2d(64, 64, kernel_size=(3, 3), padding=padd, stride=1)
        self.up_conv8 = convnext.Block(128, drop_path=drop_path1)
        self.up_conv9 = nn.Conv2d(128, 64, kernel_size=(3, 3), padding=padd, stride=1)
        self.up_conv10 = nn.Conv2d(64, 10, kernel_size=(1, 1), padding=padd2, stride=1)
        self.up_conv11 = convnext.Block(10, drop_path=drop_path1)
        self.jh = nn.ReLU()
    def forward(self, x):


        x1 = self.conv1(x)
        x1=self.bn1(x1)
        x1 = self.jh(x1)
        x2 = self.conv2(x1)
        # x2 = self.bn(x2)
        # x2 = self.jh(x2)
        x_down1 = self.down1(x2)
        # print('2', x_down1.shape)
        x3 = self.conv3(x_down1)
        x3 = self.bn3(x3)
        x3 = self.jh(x3)
        x4 = self.conv4(x3)
        # x4 = self.bn(x4)
        # x4 = self.jh(x4)
        x_down2 = self.down2(x4)
        # print('3', x_down2.shape)
        x5 = self.conv5(x_down2)
        x5 = self.bn5(x5)
        x5 = self.jh(x5)
        x6 = self.conv6(x5)
        # x6 = self.bn(x6)
        # x6 = self.jh(x6)
        x_down3 = self.down3(x6)
        # print('4', x_down3.shape)
        x7 = self.conv7(x_down3)
        x7 = self.bn7(x7)
        x7 = self.jh(x7)
        x8 = self.conv8(x7)
        # x8 = self.bn(x8)
        # x8 = self.jh(x8)
        # print('x8', x8.shape)
        x_down4 = self.down4(x8)
        # print('5', x_down4.shape)
        x9 = self.conv9(x_down4)
        x9 = self.bn9(x9)
        x9 = self.jh(x9)
        x10 = self.conv10(x9)
        # x10 = self.bn(x10)
        # x10 = self.jh(x10)
        # print('6', x10.shape)
        x_up1 = self.up1(x10)
        # print('6!', x_up1.shape)
        x_up1_cat = torch.cat((x_up1, x8), dim=1)
        # print('x_up1_cat ', x_up1_cat.shape)
        x_up1_conv1 = self.up_conv1(x_up1_cat)
        # print('x_up1_conv1', x_up1_conv1.shape)
        x_up1_conv1 = self.bn7(x_up1_conv1)
        x_up1_conv1 = self.jh(x_up1_conv1)
        x_up1_conv2 = self.up_conv2(x_up1_conv1)
        # x_up1_conv2 = self.bn(x_up1_conv2)
        # x_up1_conv2 = self.jh(x_up1_conv2)
        # print('7', x_up1_conv2.shape)
        x_up2 = self.up2(x_up1_conv2)
        x_up2_cat = torch.cat((x_up2, x6), dim=1)
        x_up2_conv3 = self.up_conv3(x_up2_cat)
        x_up2_conv3 = self.bn5(x_up2_conv3)
        x_up2_conv3 = self.jh(x_up2_conv3)
        x_up2_conv4 = self.up_conv4(x_up2_conv3)
        # x_up2_conv4 = self.bn(x_up2_conv4)
        # x_up2_conv4 = self.jh(x_up2_conv4)
        # print('8', x_up2_conv4.shape)
        x_up3 = self.up3(x_up2_conv4)
        x_up3_cat = torch.cat((x_up3, x4), dim=1)
        x_up3_conv5 = self.up_conv5(x_up3_cat)
        x_up3_conv5 = self.bn3(x_up3_conv5)
        x_up3_conv5 = self.jh(x_up3_conv5)
        x_up3_conv6 = self.up_conv6(x_up3_conv5)
        # x_up3_conv6 = self.bn(x_up3_conv6)
        # x_up3_conv6 = self.jh(x_up3_conv6)
        # print('9', x_up3_conv6.shape)
        x_up4 = self.up4(x_up3_conv6)
        x_up4_cat = torch.cat((x_up4, x2), dim=1)
        x_up4_conv7 = self.up_conv7(x_up4_cat)
        x_up4_conv7 = self.bn1(x_up4_conv7)

        x_up4_conv7 = self.jh(x_up4_conv7)
        x_up4_conv8 = self.up_conv8(x_up4_conv7)
        # x_up4_conv8 = self.bn(x_up4_conv8)

        # x_up4_conv8 = self.jh(x_up4_conv8)
        x_up4_conv9 = self.up_conv9(x_up4_conv8)
        x_up4_conv9 = self.bn0(x_up4_conv9)

        x_up4_conv9 = self.jh(x_up4_conv9)
        x_up4_conv10 = self.up_conv10(x_up4_conv9)
        x_up4_conv10= self.bn00(x_up4_conv10)

        x_up4_conv10 = self.jh(x_up4_conv10)
        x_up4_conv11 = self.up_conv11(x_up4_conv10)
        # print(x_up4_conv11.shape, x_up4_conv10.shape, x_up4_conv9.shape, x_up4_conv8.shape)
        return x_up4_conv11
